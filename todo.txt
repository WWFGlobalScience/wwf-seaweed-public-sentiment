TODO:
x Ingest the PDF list
x DB "clean" script, to de-duplicate
x de-duplicate headlines
x apply headline sentiment to all headlines
x Ingest the froueleich headlines w/ sentiment
x parse out year
x report headline sentiment
    x summary in presentation
    x raw results to whomever wants them


x parse through Dan's word doc papers for the up-to-date info, that's on bigboi under "data" now
     do sentiment tagging on the headlines
    * also, can i location tag?
    * relevance tag for "aquaculture" vs "seaweed aquaculture"?

x refactor to use transformer pipeline (wasn't any better than the batching i had before)

x read the paper
    x what method did they use for sentiment analysis?
        x authors did independent, then resolved to consensus if they disagreed
        x there's also an analysis on government public documents they did that seems less relevant, I'll just focus on headlines unless told otherwise

x read about HF sentiment analysis
    x can i add dictionary?
        x not sure how this was used, i think maybe in the gov't documents
    x can i do transfer learning?
        x yes, following this method: https://huggingface.co/blog/sentiment-analysis-python


x what i want to do in my script
    x can i start from a better base?
    x can i make the output prettier (neg, pos, neu)
    x can i do more kinds of training to get a better fit?
    * can i do a k-fold?
    x can i examine the mis-matches better?

x apply result to paper for training

* proportion of total headlines that were good and remove all duplicates does the proportion change.
* country location
    * except for US do the state

x Database design
    x headline
    x body
    x date
    x industry
    x geographic region
    x headline-sentiment
    x headline-sentiment score
    x bodysubject

* Pipeline
    * ingest articles from .docx or .pdf and dump to database?
        * check for duplication here, articles with same headline w/in 7 days of each other?


* stats
    * number of scraped articles from internet archive
        * number of regular expression matched articles
            * unique ones
                * seaweed
                * other aquaculture
    * Number of sources from dan
        * unique ones
            * seaweed
            * other aquaculture
    * Froelich count

Filter by tag
    * Froelich
    * The two sources dan got
    * Internet Archive

Output for relevant articles:
* Headline
* Publication
* Date (year?)
* Headline Sentiment
* Body subject
* Body geographic location(s)

Note that none of the articles from the internet archive had a publication tag
on them, sometimes the publications are in the "title" field, sometimes not,
and sometimes it's just not even a headline.

There's also the issue of the quality of the models. The confusion matrices for the headlines and body subject are above, I found seaweed to be such a poor fit, I now just classify as aquaculture/irrelevant, then flag it as seaweed if it matches your regular expression -- not the worst idea.